
# coding: utf-8

import os
import operator
import sys
import argparse
import torch
import numpy as np
import pickle
from utils import *

#load classification model
sys.path.insert(0,'path/HypoBertClas')
import warnings
from torch.utils.data import DataLoader
from pybert.io.dataset import CreateDataset
from pybert.io.data_transformer import DataTransformer
from pybert.utils.logginger import init_logger
from pybert.utils.utils import seed_everything
from pybert.config.basic_config import configs as config1
from pybert.model.nn.bert_fine import BertFine
from pybert.test.predicter import Predicter
from pybert.preprocessing.preprocessor import EnglishPreProcessor
from pytorch_pretrained_bert.tokenization import BertTokenizer
warnings.filterwarnings("ignore")


os.environ['CUDA_VISIBLE_DEVICES'] = '0'
device = 'cuda:%d' % config1['train']['n_gpu'][0] if len(config1['train']['n_gpu']) else 'cpu'
seed_everything(seed=config1['train']['seed'],device=device)
id2label = {value: key for key, value in config1['label2id'].items()}
# **************************** 模型 ***********************
s1_model = BertFine.from_pretrained(config1['pretrained']['bert']['bert_model_dir'],
                                 cache_dir=config1['output']['cache_dir'],
                                 num_classes = len(id2label))
# **************************** training model ***********************



logger = init_logger(log_name=config1['model']['arch'], log_dir=config1['output']['log_dir'])
predicter = Predicter(model = s1_model,
                     logger = logger,
                     n_gpu=config1['train']['n_gpu'],
                     model_path = config1['output']['checkpoint_dir'] + "/best_" + config1['model']['arch'] + "_model.pth",

                     )


# load comet model

sys.path.append(os.getcwd())

import src.data.data as data
import src.data.config as cfg
import src.interactive.functions as interactive

import spacy
nlp = spacy.load("en_core_web_lg")
cfg.device = 'cuda'
model_file =  'pretrained_models/conceptnet_pretrained_model.pickle'
opt, state_dict = interactive.load_model_file(model_file)
data_loader, text_encoder = interactive.load_data("conceptnet", opt)

n_ctx = data_loader.max_e1 + data_loader.max_e2 + data_loader.max_r
n_vocab = len(text_encoder.encoder) + n_ctx

model = interactive.make_model(opt, n_vocab, n_ctx, state_dict)


model.cuda()


#load reverse comet model
sys.path.insert(0,'path/hyperbole_gen/comet-dataset/')
import reversesrc.data.data as reverse_data
import reversesrc.data.config as reverse_cfg
import reversesrc.interactive.functions as reverse_interactive
reverse_cfg.device = 'cuda'
reverse_model_file =  '/conceptnet-generation/iteration-5000-100001/transformer/rel_language-trainsize_100-devversion_12-maxe1_10-maxe2_15-maxr_5/model_transformer-nL_12-nH_12-hSize_768-edpt_0.1-adpt_0.1-rdpt_0.1-odpt_0.1-pt_gpt-afn_gelu-init_pt-vSize_40543/exp_generation-seed_123-l2_0.01-vl2_T-lrsched_warmup_linear-lrwarm_0.002-clip_1-loss_nll-b2_0.999-b1_0.9-e_1e-08/bs_1-smax_40-sample_greedy-numseq_1-gs_full-es_full/1e-05_adam_32_20000.pickle'
reverse_opt, reverse_state_dict = reverse_interactive.load_model_file(reverse_model_file)
reverse_data_loader, reverse_text_encoder = reverse_interactive.load_data("conceptnet", reverse_opt)

reverse_n_ctx = reverse_data_loader.max_e1 + reverse_data_loader.max_e2 + reverse_data_loader.max_r
reverse_n_vocab = len(reverse_text_encoder.encoder) + reverse_n_ctx

reverse_model = reverse_interactive.make_model(reverse_opt, 40543, 29, reverse_state_dict)
reverse_model.cuda()


'''
We precompute and save as dictionary the partial results generated by reverse comet model in attri.pkl to save time.
If the attribute you want to use generate objects is not listed in attri.pkl, please call reverse_property(literal_input)['HasProperty']['beams'] or simile_vehicle(literal input). 
An example of the literal input can be 'your drawing is so bright'.
'''
attri = pickle.load(open('attri.pkl','rb'))


#Generate hyperbole candidates
prompt = "The deadly cold in Arctic region is so intimidating" 
gen = []
all_B = []
all_C = []
all_s2 = []
all_s3 = []
all_s4 = []

prompt = prompt.strip('\n')
#parse, get adj and subject
subject, adj, input_event = parse(prompt)
adj = ' and '.join(adj)

#get possible NNs (B):
NNs = getPred(prompt, relation='RelatedTo', prnt = False, sampling_algorithm = 'beam-5')
NNs = NNs['RelatedTo']['beams']
NNs += attri[adj]

#get possible actions(C):
#Causal relationship, gen from A
causal_relations = ['CausesDesire', 'HasFirstSubevent', 'HasLastSubevent']
result = getPred(input_event, relation=causal_relations, prnt = False, sampling_algorithm = 'beam-5')
movements = []
for i in range(len(causal_relations)):
    for j in range(5):
        if ' you' not in result[causal_relations[i]]['beams'][j]:
            #print(relations[i], result[relations[i]]['beams'][j])
            movements.append(result[causal_relations[i]]['beams'][j])
movements = list(set(movements))

#clean movements
fit_dict = {}
for m in movements:
    p = getprob(m, adj, relation = 'RelatedTo', prnt = False)
    fit_dict[m] = p
sorted_l = sorted(fit_dict.items(), key=operator.itemgetter(1))
num = int(len(sorted_l)*1/2)
good_movements = list(dict(sorted_l[:num]).keys())
NNs =set(NNs)
good_movements = set(good_movements)
for B in NNs:
    for C in good_movements:
        all_B.append(B)
        all_C.append(C)
#Characteristic action
for B in NNs:
    try:
        buff = set(list(get_characteristic(B)))
        for C in buff:
            all_B.append(B)
            all_C.append(C)
    except:
        continue


gen, all_s2, all_s3, all_s4 = get_score(prompt, all_B, all_C, gen, all_s2, all_s3, all_s4)
wf = open('inference_' + prompt + '.txt','w')
wf.write('\t'.join(['id','text'])+'\n')
for i, g in enumerate(gen):
    print(g)
    wf.write(str(i)+'\t'+g+'\n')
wf.close()


#get s1 score
raw_data_path = 'inference_' + prompt + '.txt'

DT = DataTransformer(logger = logger,seed = config1['train']['seed'])
targets, sentences,ids = DT.read_data(raw_data_path= raw_data_path,
                                  preprocessor=EnglishPreProcessor(),
                                  is_train=False)
tokenizer = BertTokenizer(vocab_file=config1['pretrained']['bert']['vocab_path'],
                          do_lower_case=config1['train']['do_lower_case'])

test_dataset   = CreateDataset(data  = list(zip(sentences,targets)),
                               tokenizer = tokenizer,
                               max_seq_len = config1['train']['max_seq_len'],
                               seed = config1['train']['seed'],
                               example_type = 'test')
test_loader = DataLoader(dataset     = test_dataset,
                         batch_size  = config1['train']['batch_size'],
                         num_workers = config1['train']['num_workers'],
                         shuffle     = False,
                         drop_last   = False,
                         pin_memory  = False)



logger.info('model predicting....')
result = predicter.predict(data = test_loader)


#s1, s2, s3, and s4 for classification/ranking
pkl_filename = "MLP_model.pkl"
s1 = result[:,0]
s2 = np.array(all_s2)
s3 = np.array(all_s3)
s4 = np.array(all_s4)
temp = np.array([s1, s2, s3, s4])
feat = temp.T
scaler = MinMaxScaler()
scaler.fit(feat)
norm_feat = scaler.transform(feat)
clf = pickle.load(open(pkl_filename, 'rb'))
clf = clf.predict_proba(norm_feat)[:,1]
